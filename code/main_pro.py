import os
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"
os.chdir(os.path.join(os.path.dirname(os.path.abspath(__file__)),'data'))
import gzip
import json
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModel
from tqdm import tqdm
import faiss  # å¿…é¡»å®‰è£…: pip install faiss-cpu

# ==========================================
# 1. é…ç½®å‚æ•° (é’ˆå¯¹ 3060 Laptop 6GB ä¼˜åŒ–)
# ==========================================
class Config:
    # ç¤ºä¾‹æ–‡ä»¶å (è¯·æ ¹æ®å®é™…ä¸‹è½½çš„æ–‡ä»¶åä¿®æ”¹)
    # 2023ç‰ˆé€šå¸¸åç¼€æ˜¯ .jsonl.gz
    DATA_PATH = 'Health_and_Personal_Care.jsonl.gz'       
    META_PATH = 'meta_Health_and_Personal_Care.jsonl.gz'    
    
    MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'
    
    MAX_LEN = 64  
    HISTORY_LEN = 10 
    BATCH_SIZE = 64
    EPOCHS = 10     
    LR = 1e-3
    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    EMBED_DIM = 384
    RANK = 32         
    TEMPERATURE = 0.05

config = Config()
print(f"ğŸš€ Device: {config.DEVICE} | GPU Mem: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB" if torch.cuda.is_available() else "CPU")

# ==========================================
# 2. æ•°æ®å¤„ç† (åºåˆ—åŒ–)
# ==========================================
# ==========================================
# é€šç”¨ Metadata å¤„ç†å·¥å…·å‡½æ•° (æ–°å¢)
# ==========================================
def format_metadata_to_text(meta_item, max_len_chars=300):
    """
    é’ˆå¯¹ Amazon Reviews 2023 æ•°æ®é›†çš„æ–‡æœ¬åºåˆ—åŒ–å‡½æ•° (ä¿®å¤ NoneType æŠ¥é”™ç‰ˆ)
    """
    parts = []
    
    # è¾…åŠ©å‡½æ•°ï¼šå®‰å…¨è·å–å­—ç¬¦ä¸²ï¼Œå¤„ç† None å’Œ 'nan'
    def safe_get(key):
        val = meta_item.get(key)
        if val is None: return ""
        val = str(val).strip()
        if val.lower() == 'nan': return ""
        return val

    # 1. æ ¸å¿ƒå­—æ®µ (ä¼˜å…ˆçº§æœ€é«˜)
    title = safe_get('title')
    if title:
        parts.append(f"Title: {title}")

    # 2. å“ç‰Œ/åº—é“ºä¿¡æ¯
    store = safe_get('store')
    if store:
        parts.append(f"Brand: {store}")

    # 3. ä¸»è¦åˆ†ç±»
    cat = safe_get('main_category')
    if cat and cat.lower() != 'all categories':
        parts.append(f"Category: {cat}")

    # 4. è¯¦ç»†å‚æ•° (Details)
    details = meta_item.get('details')
    if isinstance(details, dict):
        # æŒ‘é€‰ä¸€äº›é€šç”¨çš„é«˜ä»·å€¼ Key
        valid_keys = ['author', 'artist', 'brand', 'format', 'color', 'genre', 'label']
        for k, v in details.items():
            if not k or not v: continue # è·³è¿‡ç©ºé”®å€¼
            k_lower = str(k).lower()
            # æ¨¡ç³ŠåŒ¹é… key
            if any(vk in k_lower for vk in valid_keys):
                seg = f"{k}: {str(v).strip()}"
                if len(parts) < 8: # é˜²æ­¢å¤ªé•¿
                    parts.append(seg)
    
    # 5. ç‰¹æ€§åˆ—è¡¨ (Features)
    features = meta_item.get('features')
    if isinstance(features, list) and features:
        # åªå–å‰ 2 ä¸ªç‰¹æ€§
        count = 0
        for feat in features:
            if count >= 2: break
            if feat:
                feat_str = str(feat).strip()
                if feat_str:
                    parts.append(f"Feature: {feat_str}")
                    count += 1

    # 6. æè¿° (Description)
    desc = meta_item.get('description')
    desc_text = ""
    if isinstance(desc, list) and len(desc) > 0:
        desc_text = str(desc[0])
    elif isinstance(desc, str):
        desc_text = desc
    
    if desc_text:
        # ç®€å•æˆªæ–­
        clean_desc = desc_text.strip()[:100]
        if clean_desc:
            parts.append(f"Desc: {clean_desc}...")

    # 7. æ™ºèƒ½æ‹¼æ¥ä¸æˆªæ–­
    final_text = ""
    for part in parts:
        # é¢„ä¼°æ·»åŠ åçš„é•¿åº¦
        if len(final_text) + len(part) > max_len_chars:
            # å¦‚æœ Title è¿˜æ²¡åŠ è¿›å»ï¼Œç¡¬å¡
            if "Title:" in part and "Title:" not in final_text:
                remaining = max_len_chars - len(final_text)
                if remaining > 10:
                    final_text += part[:remaining]
            break
        final_text += part + " ; "
    
    return final_text.strip(" ; ")

def load_and_process_data(review_path, meta_path, limit=None):
    print(f"Loading 2023 Dataset...")
    print(f"Meta: {meta_path}")
    print(f"Review: {review_path}")
    
    # --- 1. åŠ è½½ Metadata ---
    # 2023 ç‰ˆ Key: parent_asin
    asin2text = {} 
    
    meta_count = 0
    with gzip.open(meta_path, 'r') as f:
        for l in tqdm(f, desc="Reading Meta"):
            try:
                # 2023 æ•°æ®é›†æ˜¯æ ‡å‡†çš„ JSONLï¼Œç›´æ¥ json.loads å³å¯ï¼Œä¸éœ€è¦ eval
                line = json.loads(l.strip())
                
                # ä½¿ç”¨ parent_asin ä½œä¸ºå”¯ä¸€æ ‡è¯† (èšåˆå˜ä½“)
                # å¦‚æœæ²¡æœ‰ parent_asinï¼Œå°è¯•ç”¨ asin
                item_id = line.get('parent_asin', line.get('asin'))
                
                if not item_id: continue
                
                processed_text = format_metadata_to_text(line)
                if processed_text:
                    asin2text[item_id] = processed_text
                    meta_count += 1
            except json.JSONDecodeError:
                continue

    print(f"âœ… Loaded {len(asin2text)} items metadata.")

    # --- 2. åŠ è½½ Review æ•°æ® ---
    # 2023 ç‰ˆ Key: user_id, parent_asin, timestamp
    data = []
    hit_meta_count = 0 
    
    with gzip.open(review_path, 'r') as f:
        for i, l in enumerate(tqdm(f, desc="Reading Reviews")):
            if limit and i >= limit: break
            try:
                line = json.loads(l.strip())
                
                # ID æ˜ å°„
                user_id = line.get('user_id')
                item_id = line.get('parent_asin', line.get('asin'))
                timestamp = line.get('timestamp', 0)
                
                if not user_id or not item_id: continue

                # ä¼˜å…ˆä½¿ç”¨ Meta
                if item_id in asin2text:
                    final_text = asin2text[item_id]
                    hit_meta_count += 1
                else:
                    # å…œåº•ï¼šä½¿ç”¨ Review é‡Œçš„ title
                    # 2023 ç‰ˆ review é‡Œæœ‰ 'title' å’Œ 'text'
                    parts = []
                    if 'title' in line: parts.append(line['title'])
                    if 'text' in line: parts.append(line['text'][:100])
                    final_text = " ".join(parts)
                
                if len(final_text) < 5: continue

                data.append({
                    'user': user_id,
                    'item': item_id,
                    'text': final_text, 
                    'time': timestamp
                })
            except json.JSONDecodeError:
                continue
    
    hit_rate = hit_meta_count / len(data) if len(data) > 0 else 0
    print(f"ğŸ“Š Metadata Hit Rate: {hit_rate:.2%}")
    
    df = pd.DataFrame(data)
    # 2023 timestamp å¯èƒ½æ˜¯æ¯«ç§’ï¼Œæ’åºé€»è¾‘ä¸å˜
    df = df.sort_values(['user', 'time'])
    return df
# åŠ è½½æ•°æ® (é™åˆ¶ 50k æ¡ç”¨äºæ¼”ç¤ºï¼Œè·‘å…¨é‡å¯å»æ‰ limit)
full_df = load_and_process_data(config.DATA_PATH, config.META_PATH, limit=100000)

print("æ„å»º Item æ˜ å°„...")
item_list = full_df['item'].unique()
item_map = {asin: i for i, asin in enumerate(item_list)}
id2item_text = {i: text for asin, text, i in zip(full_df['item'], full_df['text'], [item_map[a] for a in full_df['item']])}
# è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„ text mapï¼Œå®é™…ä¸Šä¸€ä¸ª asin å¯èƒ½æœ‰å¤šæ¡è¯„è®ºï¼Œè¿™é‡Œéšæœºå–äº†ä¸€æ¡ä½œä¸ºè¯¥ç‰©å“çš„ä»£è¡¨æ–‡æœ¬
# ç”Ÿäº§ç¯å¢ƒä¸­åº”è¯¥å»ºç«‹ä¸“é—¨çš„ Item Meta è¡¨

full_df['item_idx'] = full_df['item'].map(item_map)
NUM_ITEMS = len(item_list)
print(f"Total Interactions: {len(full_df)}, Total Items: {NUM_ITEMS}")

# æ„å»ºåºåˆ—æ•°æ®
# æ ¼å¼: ([history_item_texts], target_item_text)
train_samples = []
test_samples = []

print("æ„å»ºç”¨æˆ·å†å²åºåˆ—...")
user_groups = full_df.groupby('user')
for uid, group in tqdm(user_groups):
    if len(group) < 3: continue # äº¤äº’å¤ªå°‘æ— æ³•æ„å»ºåºåˆ—
    
    items = group['item_idx'].tolist()
    texts = group['text'].tolist()
    
    # ç®€å•çš„ Leave-one-out åˆ’åˆ†
    # å€’æ•°ç¬¬1ä¸ªæ˜¯æµ‹è¯•é›†ç›®æ ‡ï¼Œå€’æ•°ç¬¬2ä¸ªæ˜¯æµ‹è¯•é›† Seedï¼Œå€’æ•° 2-N æ˜¯æµ‹è¯•é›†å†å²
    
    # --- æ„å»ºæµ‹è¯•æ ·æœ¬ ---
    # XPERT é€»è¾‘ï¼šSeed Event æ˜¯è§¦å‘æ£€ç´¢çš„äº‹ä»¶ã€‚
    # è¿™é‡Œæˆ‘ä»¬å®šä¹‰ï¼šInput History ç”¨äºç”Ÿæˆ Morphï¼ŒLast Item in History ä½œä¸º Seed Event
    
    # æµ‹è¯•é›†ï¼šç”¨è¿‡å»çš„æ‰€æœ‰æ•°æ®é¢„æµ‹æœ€åä¸€ä¸ª
    if len(items) > config.HISTORY_LEN:
        hist_texts = texts[-(config.HISTORY_LEN+1):-1] # å–å€’æ•°ç¬¬2ä¸ªå¾€å‰æ¨Nä¸ª
        target_text = texts[-1]
        target_item_idx = items[-1]
        test_samples.append((hist_texts, target_text, target_item_idx))
    
    # è®­ç»ƒé›†ï¼šæ»‘åŠ¨çª—å£
    # æ¯”å¦‚åºåˆ— A, B, C, D, E
    # (A,B)->C, (A,B,C)->D...
    for i in range(1, len(items)-1):
        # çª—å£æˆªæ­¢åˆ° i
        start = max(0, i - config.HISTORY_LEN)
        hist_window = texts[start:i+1] # åŒ…å« seed event (ç¬¬ i ä¸ª)
        target = texts[i+1]
        train_samples.append((hist_window, target))

print(f"Train Samples: {len(train_samples)}, Test Samples: {len(test_samples)}")

# ==========================================
# 3. Dataset & DataLoader
# ==========================================
class SeqRecDataset(Dataset):
    def __init__(self, samples, tokenizer, max_len, is_test=False):
        self.samples = samples
        self.tokenizer = tokenizer
        self.max_len = max_len
        self.is_test = is_test

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        if self.is_test:
            hist_texts, target_text, target_idx = self.samples[idx]
        else:
            hist_texts, target_text = self.samples[idx]
            target_idx = -1

        # Seed Event æ˜¯å†å²é‡Œçš„æœ€åä¸€ä¸ª
        seed_text = hist_texts[-1]
        # ç”¨äºç”Ÿæˆ User Preference çš„æ˜¯å†å² (è¿™é‡Œç®€å•èµ·è§ï¼ŒæŠŠ seed ä¹Ÿæ”¾è¿› history ç¼–ç )
        context_text = " [SEP] ".join(hist_texts) 

        return context_text, seed_text, target_text, target_idx

def collate_fn(batch):
    context_list, seed_list, target_list, idx_list = zip(*batch)
    
    # ç»Ÿä¸€ Tokenize
    def tokenize(text_list):
        return tokenizer(
            list(text_list), 
            padding='max_length', truncation=True, max_length=config.MAX_LEN, return_tensors='pt'
        )
    
    return {
        'context': tokenize(context_list), # ç”¨äº LSTM ç”Ÿæˆ Morph
        'seed': tokenize(seed_list),       # ç”¨äºè¢« Morph ä½œç”¨ (åŸºå‡†ç‚¹)
        'target': tokenize(target_list),   # æ­£æ ·æœ¬
        'target_idx': torch.tensor(idx_list)
    }

tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME)
train_dataset = SeqRecDataset(train_samples, tokenizer, config.MAX_LEN)
train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=0) # Windowsä¸‹è®¾0

# ==========================================
# 4. Proç‰ˆ æ¨¡å‹å®šä¹‰ (Low-Rank + LSTM)
# ==========================================
class XpertPro(nn.Module):
    def __init__(self, model_name, embed_dim, rank, hidden_dim=256):
        super(XpertPro, self).__init__()
        
        # 1. æ–‡æœ¬ç¼–ç å™¨ (Freeze å†»ç»“ä»¥èŠ‚çœæ˜¾å­˜)
        self.text_encoder = AutoModel.from_pretrained(model_name)
        #è®¾ä¸ºFalseæ—¶å†»ç»“å‚æ•°
        for param in self.text_encoder.parameters():
            param.requires_grad = True
            
        # 2. åå¥½æå–å™¨ (LSTM)
        # è¾“å…¥æ˜¯ Generic Text Embedding, è¾“å‡ºæ˜¯ User State
        self.preference_rnn = nn.LSTM(
            input_size=embed_dim, 
            hidden_size=hidden_dim, 
            num_layers=1, 
            batch_first=True
        )
        
        # 3. ä½ç§©çŸ©é˜µç”Ÿæˆå™¨ (Low-Rank Generator)
        # ç”ŸæˆçŸ©é˜µ A (D x r) å’Œ B (D x r)
        # è¾“å‡ºç»´åº¦ = D * r
        self.head_A = nn.Linear(hidden_dim, embed_dim * rank)
        self.head_B = nn.Linear(hidden_dim, embed_dim * rank)
        
        self.embed_dim = embed_dim
        self.rank = rank

    def get_generic_embedding(self, inputs):
        # ä»…æ¨ç†ï¼Œä¸è®¡ç®—æ¢¯åº¦
        with torch.no_grad():
            outputs = self.text_encoder(**inputs)
        # Mean Pooling
        emb = outputs.last_hidden_state.mean(dim=1)
        return F.normalize(emb, p=2, dim=1)

    def generate_morph_operators(self, context_emb):
        # context_emb: [batch, dim] (è¿™é‡Œç®€åŒ–äº†ï¼Œç›´æ¥æŠŠ concat çš„ history ä½œä¸ºä¸€ä¸ª embedding å–‚ç»™ LSTM çš„ä¸€æ­¥)
        # å¦‚æœè¿½æ±‚æ›´ç²¾ç»†ï¼Œåº”è¯¥æŠŠ history åˆ†å¼€ tokenizeï¼Œå¾—åˆ° [batch, seq, dim]ï¼Œç„¶åå–‚ç»™ LSTM
        # ä¸ºäº†é€Ÿåº¦å’Œæ˜¾å­˜ï¼Œè¿™é‡Œè¾“å…¥æ˜¯ [batch, 1, dim]
        
        _, (h_n, _) = self.preference_rnn(context_emb.unsqueeze(1))
        user_state = h_n.squeeze(0) # [batch, hidden_dim]
        
        # ç”Ÿæˆ A å’Œ B
        batch_size = user_state.size(0)
        
        # A: [batch, D, r]
        mat_A = self.head_A(user_state).view(batch_size, self.embed_dim, self.rank)
        # B: [batch, D, r]
        mat_B = self.head_B(user_state).view(batch_size, self.embed_dim, self.rank)
        
        return mat_A, mat_B

    def forward(self, context_inputs, seed_inputs, target_inputs=None):
        # 1. è·å–æ‰€æœ‰ Generic Embeddings
        # context: ç”¨æˆ·å†å²æ–‡æœ¬æ‹¼æ¥
        # seed: è§¦å‘æ£€ç´¢çš„é‚£ä¸ªç‰©å“
        # target: çœŸå®ç‚¹å‡»çš„ä¸‹ä¸€ä¸ªç‰©å“
        
        context_emb = self.get_generic_embedding(context_inputs)
        seed_emb = self.get_generic_embedding(seed_inputs)
        
        # 2. ç”Ÿæˆ Low-Rank Morph Operators
        # P_u = I + A @ B.T
        mat_A, mat_B = self.generate_morph_operators(context_emb)
        
        # 3. åº”ç”¨ Morph Operator (æ ¸å¿ƒä¼˜åŒ–)
        # æˆ‘ä»¬éœ€è¦è®¡ç®— v_pers = P_u @ v_seed
        # v_pers = (I + A @ B.T) @ v_seed = v_seed + A @ (B.T @ v_seed)
        # è¿™æ ·è®¡ç®—å¤æ‚åº¦ä» O(D^2) é™åˆ° O(D*r)
        
        # seed_emb: [batch, D, 1]
        v_seed = seed_emb.unsqueeze(2) 
        
        # step 1: temp = B.T @ v_seed -> [batch, r, 1]
        temp = torch.bmm(mat_B.transpose(1, 2), v_seed)
        
        # step 2: delta = A @ temp -> [batch, D, 1]
        delta = torch.bmm(mat_A, temp).squeeze(2)
        
        # step 3: res = v_seed + delta
        personalized_query = F.normalize(seed_emb + delta, p=2, dim=1)
        
        if target_inputs is not None:
            target_emb = self.get_generic_embedding(target_inputs)
            return personalized_query, target_emb
        else:
            return personalized_query

model = XpertPro(config.MODEL_NAME, config.EMBED_DIM, config.RANK)
model.to(config.DEVICE)

# ==========================================
# 5. è®­ç»ƒ (Training)
# ==========================================
optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=config.LR)
scaler = torch.cuda.amp.GradScaler()

print(">>> Start Training (Freeze BERT, Train Adapter Only)...")

for epoch in range(config.EPOCHS):
    model.train()
    total_loss = 0
    pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}")
    
    for batch in pbar:
        # Move to device
        ctx = {k: v.to(config.DEVICE) for k, v in batch['context'].items()}
        seed = {k: v.to(config.DEVICE) for k, v in batch['seed'].items()}
        tgt = {k: v.to(config.DEVICE) for k, v in batch['target'].items()}
        
        optimizer.zero_grad()
        
        with torch.cuda.amp.autocast():
            # query: ä¸ªæ€§åŒ–åçš„å‘é‡, key: ç›®æ ‡ç‰©å“çš„é€šç”¨å‘é‡
            query_emb, key_emb = model(ctx, seed, tgt)
            
            # InfoNCE Loss (Contrastive)
            # åŒæ · batch å†…çš„å…¶ä»– target ä½œä¸ºè´Ÿæ ·æœ¬
            logits = torch.matmul(query_emb, key_emb.T) / config.TEMPERATURE
            labels = torch.arange(logits.size(0)).long().to(config.DEVICE)
            loss = nn.CrossEntropyLoss()(logits, labels)
            
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        
        total_loss += loss.item()
        pbar.set_postfix({'loss': f"{loss.item():.4f}"})

# ==========================================
# 6. FAISS å…¨é‡è¯„ä¼° (Recall@K)
# ==========================================
print("\n>>> Building Index for Evaluation (Recall@50)...")
model.eval()

# 6.1 è®¡ç®—æ‰€æœ‰ Item çš„ Embedding (Generic) å»ºç«‹ç´¢å¼•
all_item_texts = [id2item_text[i] for i in range(NUM_ITEMS)]
item_embs = []

# åˆ†æ‰¹è®¡ç®— Item Embedding
BATCH_EVAL = 128
with torch.no_grad():
    for i in tqdm(range(0, NUM_ITEMS, BATCH_EVAL), desc="Encoding Items"):
        batch_texts = all_item_texts[i : i + BATCH_EVAL]
        inputs = tokenizer(batch_texts, padding='max_length', truncation=True, max_length=config.MAX_LEN, return_tensors='pt').to(config.DEVICE)
        emb = model.get_generic_embedding(inputs)
        item_embs.append(emb.cpu().numpy())

item_matrix = np.concatenate(item_embs, axis=0) # [Num_Items, 384]

# 6.2 å»ºç«‹ FAISS ç´¢å¼• (Inner Product)
index = faiss.IndexFlatIP(config.EMBED_DIM)
index.add(item_matrix)
print(f"FAISS Index Built: {index.ntotal} items.")

# 6.3 è®¡ç®—æµ‹è¯•é›†ç”¨æˆ·çš„ Personalized Query
test_dataset = SeqRecDataset(test_samples, tokenizer, config.MAX_LEN, is_test=True)
test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, collate_fn=collate_fn, num_workers=0)

hits_10 = 0
hits_50 = 0
total_test = 0

print(">>> Running Retrieval Evaluation...")
with torch.no_grad():
    for batch in tqdm(test_loader):
        ctx = {k: v.to(config.DEVICE) for k, v in batch['context'].items()}
        seed = {k: v.to(config.DEVICE) for k, v in batch['seed'].items()}
        target_indices = batch['target_idx'].numpy()
        
        # ç”Ÿæˆä¸ªæ€§åŒ–æŸ¥è¯¢å‘é‡
        query_vecs = model(ctx, seed).cpu().numpy() # [batch, 384]
        
        # æœç´¢ Top-K
        D, I = index.search(query_vecs, 50) # I: [batch, 50]
        
        # è®¡ç®— Hit
        for rank, target_idx in zip(I, target_indices):
            if target_idx in rank[:10]:
                hits_10 += 1
            if target_idx in rank[:50]:
                hits_50 += 1
        
        total_test += len(target_indices)

print("="*40)
print(f"ğŸ“Š Final Evaluation Results (Test Set Size: {total_test})")
print(f"Recall@10: {hits_10 / total_test:.4f}")
print(f"Recall@50: {hits_50 / total_test:.4f}")
print("="*40)

# ==========================================
# 7. æ¡ˆä¾‹å±•ç¤º (Qualitative)
# ==========================================
print("\n>>> Showing a Personalized Case...")
# æ‰¾ä¸€ä¸ªæµ‹è¯•æ ·æœ¬
sample_idx = 0
sample_ctx, sample_seed, sample_target, _ = test_dataset[sample_idx]

print(f"ğŸ‘¤ User History Context: {sample_ctx[:80]}...")
print(f"ğŸŒ± Seed Item: {sample_seed[:50]}...")
print(f"ğŸ¯ True Target: {sample_target[:50]}...")

# æ¨¡æ‹Ÿæ¨ç†
inputs_ctx = tokenizer([sample_ctx], padding='max_length',truncation=True, max_length=config.MAX_LEN, return_tensors='pt').to(config.DEVICE)
inputs_seed = tokenizer([sample_seed], padding='max_length', max_length=config.MAX_LEN, return_tensors='pt').to(config.DEVICE)

with torch.no_grad():
    # 1. é€šç”¨å‘é‡æ£€ç´¢ç»“æœ
    gen_emb = model.get_generic_embedding(inputs_seed).cpu().numpy()
    _, I_gen = index.search(gen_emb, 5)
    print("\n[Generic Retrieval (Without Morph)]:")
    for idx in I_gen[0]:
        print(f" - {id2item_text[idx][:60]}")
        
    # 2. ä¸ªæ€§åŒ–å‘é‡æ£€ç´¢ç»“æœ
    pers_emb = model(inputs_ctx, inputs_seed).cpu().numpy()
    _, I_pers = index.search(pers_emb, 5)
    print("\n[XPERT Retrieval (With Low-Rank Morph)]:")
    for idx in I_pers[0]:
        print(f" - {id2item_text[idx][:60]}")